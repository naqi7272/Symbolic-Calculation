{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Common Task 1:** Dataset preprocessing\n",
        "\n",
        "**Description:** Use **Sympy or Mathematica** to generate datasets of functions with their Taylor expansions up the fourth order. Tokenize the dataset."
      ],
      "metadata": {
        "id": "T4TNsGvaLvTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sympy as sp\n",
        "\n",
        "x = sp.symbols('x')\n",
        "\n",
        "functions = [sp.sin(x), sp.exp(x), sp.ln(1 + x), sp.cos(x), sp.tan(x)]\n",
        "\n",
        "taylor_data = {}\n",
        "\n",
        "for func in functions:\n",
        "    taylor_series = sp.series(func, x, 0, 5).removeO()\n",
        "    taylor_data[str(func)] = str(taylor_series)\n",
        "\n",
        "for func, taylor in taylor_data.items():\n",
        "    print(f\"Function: {func}\\nTaylor Expansion: {taylor}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y23-iqViEpqu",
        "outputId": "006fcada-9618-40bb-e2e1-1be3b64fab2b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function: sin(x)\n",
            "Taylor Expansion: -x**3/6 + x\n",
            "\n",
            "Function: exp(x)\n",
            "Taylor Expansion: x**4/24 + x**3/6 + x**2/2 + x + 1\n",
            "\n",
            "Function: log(x + 1)\n",
            "Taylor Expansion: -x**4/4 + x**3/3 - x**2/2 + x\n",
            "\n",
            "Function: cos(x)\n",
            "Taylor Expansion: x**4/24 - x**2/2 + 1\n",
            "\n",
            "Function: tan(x)\n",
            "Taylor Expansion: x**3/3 + x\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Common Task 2:** Use LSTM model\n",
        "\n",
        "Please train an **LSTM model** to learn the Taylor expansion of each function.\n",
        "You can use a deep learning algorithm of your choice (in Keras/TF or Pytorch).\n"
      ],
      "metadata": {
        "id": "STFJnvR3L__T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def tokenize(expression):\n",
        "    tokens = re.findall(r'\\d+|\\w+|[+\\-*/^()]', expression)\n",
        "    return tokens\n",
        "\n",
        "tokenized_data = {func: tokenize(taylor) for func, taylor in taylor_data.items()}\n",
        "\n",
        "text_sequences = [' '.join(tokens) for tokens in tokenized_data.values()]\n",
        "\n",
        "tokenizer = Tokenizer(filters='', lower=False)\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)"
      ],
      "metadata": {
        "id": "ncYn9A-4tiPh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "max_seq_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# X = np.array([seq[:-1] for seq in padded_sequences.values()])\n",
        "# y = np.array([seq[1:] for seq in padded_sequences.values()])\n",
        "X = np.array([seq[:-1] for seq in padded_sequences])\n",
        "y = np.array([seq[1:] for seq in padded_sequences])\n"
      ],
      "metadata": {
        "id": "4S5zstz8Epvu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=50, input_length=max_seq_length - 1),\n",
        "    Bidirectional(LSTM(100, return_sequences=True)),\n",
        "    Bidirectional(LSTM(100, return_sequences=True)),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=250, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPb8dqXoEpxy",
        "outputId": "bc3a20ab-c2e2-4e50-856d-9793e163a7d0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.0174 - loss: 2.4873\n",
            "Epoch 2/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.4435 - loss: 2.4617\n",
            "Epoch 3/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.4087 - loss: 2.4337\n",
            "Epoch 4/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.3913 - loss: 2.3966\n",
            "Epoch 5/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3739 - loss: 2.3437\n",
            "Epoch 6/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.3565 - loss: 2.2672\n",
            "Epoch 7/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.3565 - loss: 2.1578\n",
            "Epoch 8/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.3565 - loss: 2.0124\n",
            "Epoch 9/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.3565 - loss: 1.8693\n",
            "Epoch 10/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3565 - loss: 1.8437\n",
            "Epoch 11/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.3652 - loss: 1.8436\n",
            "Epoch 12/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.4174 - loss: 1.7600\n",
            "Epoch 13/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.4696 - loss: 1.6671\n",
            "Epoch 14/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4957 - loss: 1.6236\n",
            "Epoch 15/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5217 - loss: 1.6216\n",
            "Epoch 16/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.5043 - loss: 1.6243\n",
            "Epoch 17/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5043 - loss: 1.6073\n",
            "Epoch 18/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5043 - loss: 1.5682\n",
            "Epoch 19/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.5217 - loss: 1.5186\n",
            "Epoch 20/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.5304 - loss: 1.4761\n",
            "Epoch 21/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5304 - loss: 1.4543\n",
            "Epoch 22/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5304 - loss: 1.4503\n",
            "Epoch 23/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.5304 - loss: 1.4452\n",
            "Epoch 24/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5304 - loss: 1.4266\n",
            "Epoch 25/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5304 - loss: 1.3999\n",
            "Epoch 26/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.5391 - loss: 1.3783\n",
            "Epoch 27/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5391 - loss: 1.3675\n",
            "Epoch 28/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.5391 - loss: 1.3610\n",
            "Epoch 29/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.5478 - loss: 1.3494\n",
            "Epoch 30/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5304 - loss: 1.3337\n",
            "Epoch 31/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5130 - loss: 1.3224\n",
            "Epoch 32/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.5043 - loss: 1.3093\n",
            "Epoch 33/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5304 - loss: 1.2916\n",
            "Epoch 34/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5565 - loss: 1.2795\n",
            "Epoch 35/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5478 - loss: 1.2708\n",
            "Epoch 36/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5565 - loss: 1.2587\n",
            "Epoch 37/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5739 - loss: 1.2428\n",
            "Epoch 38/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.5739 - loss: 1.2269\n",
            "Epoch 39/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5913 - loss: 1.2117\n",
            "Epoch 40/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6000 - loss: 1.1951\n",
            "Epoch 41/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6000 - loss: 1.1817\n",
            "Epoch 42/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.5913 - loss: 1.1712\n",
            "Epoch 43/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.6087 - loss: 1.1571\n",
            "Epoch 44/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6087 - loss: 1.1441\n",
            "Epoch 45/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6087 - loss: 1.1294\n",
            "Epoch 46/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6087 - loss: 1.1133\n",
            "Epoch 47/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6087 - loss: 1.0986\n",
            "Epoch 48/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6174 - loss: 1.0816\n",
            "Epoch 49/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6174 - loss: 1.0685\n",
            "Epoch 50/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.6435 - loss: 1.0532\n",
            "Epoch 51/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.6435 - loss: 1.0382\n",
            "Epoch 52/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.6522 - loss: 1.0224\n",
            "Epoch 53/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.6435 - loss: 1.0037\n",
            "Epoch 54/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.6522 - loss: 0.9866\n",
            "Epoch 55/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.6609 - loss: 0.9723\n",
            "Epoch 56/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.6348 - loss: 0.9694\n",
            "Epoch 57/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.6174 - loss: 0.9660\n",
            "Epoch 58/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.6435 - loss: 0.9472\n",
            "Epoch 59/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.6783 - loss: 0.9120\n",
            "Epoch 60/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.6348 - loss: 0.9301\n",
            "Epoch 61/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.6696 - loss: 0.9062\n",
            "Epoch 62/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6696 - loss: 0.8795\n",
            "Epoch 63/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.6348 - loss: 0.8922\n",
            "Epoch 64/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.6783 - loss: 0.8534\n",
            "Epoch 65/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6696 - loss: 0.8559\n",
            "Epoch 66/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6870 - loss: 0.8395\n",
            "Epoch 67/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6957 - loss: 0.8200\n",
            "Epoch 68/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.6609 - loss: 0.8249\n",
            "Epoch 69/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6957 - loss: 0.7989\n",
            "Epoch 70/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7043 - loss: 0.7955\n",
            "Epoch 71/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7043 - loss: 0.7915\n",
            "Epoch 72/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.7304 - loss: 0.7676\n",
            "Epoch 73/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.6957 - loss: 0.7707\n",
            "Epoch 74/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7217 - loss: 0.7632\n",
            "Epoch 75/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7217 - loss: 0.7422\n",
            "Epoch 76/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7217 - loss: 0.7387\n",
            "Epoch 77/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.7304 - loss: 0.7378\n",
            "Epoch 78/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7391 - loss: 0.7156\n",
            "Epoch 79/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7478 - loss: 0.7076\n",
            "Epoch 80/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7478 - loss: 0.7069\n",
            "Epoch 81/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7391 - loss: 0.6952\n",
            "Epoch 82/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7565 - loss: 0.6782\n",
            "Epoch 83/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7652 - loss: 0.6732\n",
            "Epoch 84/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7565 - loss: 0.6696\n",
            "Epoch 85/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7565 - loss: 0.6604\n",
            "Epoch 86/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.7826 - loss: 0.6461\n",
            "Epoch 87/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8000 - loss: 0.6355\n",
            "Epoch 88/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7826 - loss: 0.6320\n",
            "Epoch 89/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8000 - loss: 0.6271\n",
            "Epoch 90/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.7826 - loss: 0.6212\n",
            "Epoch 91/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8000 - loss: 0.6071\n",
            "Epoch 92/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.7913 - loss: 0.5952\n",
            "Epoch 93/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7913 - loss: 0.5859\n",
            "Epoch 94/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8000 - loss: 0.5804\n",
            "Epoch 95/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8174 - loss: 0.5774\n",
            "Epoch 96/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8087 - loss: 0.5753\n",
            "Epoch 97/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8261 - loss: 0.5777\n",
            "Epoch 98/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.8174 - loss: 0.5766\n",
            "Epoch 99/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8174 - loss: 0.5686\n",
            "Epoch 100/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8174 - loss: 0.5453\n",
            "Epoch 101/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8348 - loss: 0.5246\n",
            "Epoch 102/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8522 - loss: 0.5263\n",
            "Epoch 103/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8348 - loss: 0.5268\n",
            "Epoch 104/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8609 - loss: 0.5153\n",
            "Epoch 105/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8522 - loss: 0.4963\n",
            "Epoch 106/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8696 - loss: 0.4954\n",
            "Epoch 107/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.8609 - loss: 0.4961\n",
            "Epoch 108/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.8870 - loss: 0.4830\n",
            "Epoch 109/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8870 - loss: 0.4690\n",
            "Epoch 110/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8696 - loss: 0.4659\n",
            "Epoch 111/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8957 - loss: 0.4653\n",
            "Epoch 112/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8696 - loss: 0.4558\n",
            "Epoch 113/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9043 - loss: 0.4429\n",
            "Epoch 114/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.9043 - loss: 0.4362\n",
            "Epoch 115/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9043 - loss: 0.4344\n",
            "Epoch 116/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9043 - loss: 0.4294\n",
            "Epoch 117/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9043 - loss: 0.4204\n",
            "Epoch 118/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9304 - loss: 0.4100\n",
            "Epoch 119/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9304 - loss: 0.4038\n",
            "Epoch 120/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9304 - loss: 0.4003\n",
            "Epoch 121/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9391 - loss: 0.3971\n",
            "Epoch 122/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9217 - loss: 0.3935\n",
            "Epoch 123/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9391 - loss: 0.3865\n",
            "Epoch 124/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9391 - loss: 0.3775\n",
            "Epoch 125/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9565 - loss: 0.3688\n",
            "Epoch 126/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9478 - loss: 0.3620\n",
            "Epoch 127/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9478 - loss: 0.3562\n",
            "Epoch 128/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9565 - loss: 0.3521\n",
            "Epoch 129/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9391 - loss: 0.3502\n",
            "Epoch 130/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9478 - loss: 0.3483\n",
            "Epoch 131/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9217 - loss: 0.3494\n",
            "Epoch 132/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9478 - loss: 0.3536\n",
            "Epoch 133/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9217 - loss: 0.3561\n",
            "Epoch 134/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9478 - loss: 0.3529\n",
            "Epoch 135/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9478 - loss: 0.3334\n",
            "Epoch 136/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9565 - loss: 0.3102\n",
            "Epoch 137/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9565 - loss: 0.3105\n",
            "Epoch 138/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9478 - loss: 0.3146\n",
            "Epoch 139/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9565 - loss: 0.3039\n",
            "Epoch 140/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9565 - loss: 0.2906\n",
            "Epoch 141/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - accuracy: 0.9565 - loss: 0.2910\n",
            "Epoch 142/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9652 - loss: 0.2904\n",
            "Epoch 143/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9565 - loss: 0.2779\n",
            "Epoch 144/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9565 - loss: 0.2732\n",
            "Epoch 145/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9652 - loss: 0.2753\n",
            "Epoch 146/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9565 - loss: 0.2659\n",
            "Epoch 147/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9565 - loss: 0.2583\n",
            "Epoch 148/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9652 - loss: 0.2583\n",
            "Epoch 149/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9652 - loss: 0.2534\n",
            "Epoch 150/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9652 - loss: 0.2459\n",
            "Epoch 151/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.9652 - loss: 0.2411\n",
            "Epoch 152/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9652 - loss: 0.2406\n",
            "Epoch 153/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9739 - loss: 0.2351\n",
            "Epoch 154/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.9652 - loss: 0.2287\n",
            "Epoch 155/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9652 - loss: 0.2243\n",
            "Epoch 156/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9739 - loss: 0.2224\n",
            "Epoch 157/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9652 - loss: 0.2191\n",
            "Epoch 158/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9739 - loss: 0.2135\n",
            "Epoch 159/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9739 - loss: 0.2083\n",
            "Epoch 160/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9739 - loss: 0.2049\n",
            "Epoch 161/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9913 - loss: 0.2025\n",
            "Epoch 162/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.9826 - loss: 0.1992\n",
            "Epoch 163/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9913 - loss: 0.1946\n",
            "Epoch 164/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.9913 - loss: 0.1897\n",
            "Epoch 165/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9913 - loss: 0.1864\n",
            "Epoch 166/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9913 - loss: 0.1836\n",
            "Epoch 167/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9913 - loss: 0.1808\n",
            "Epoch 168/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.9913 - loss: 0.1768\n",
            "Epoch 169/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9913 - loss: 0.1729\n",
            "Epoch 170/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9913 - loss: 0.1691\n",
            "Epoch 171/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.1663\n",
            "Epoch 172/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9913 - loss: 0.1635\n",
            "Epoch 173/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.1605\n",
            "Epoch 174/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 0.1570\n",
            "Epoch 175/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 1.0000 - loss: 0.1536\n",
            "Epoch 176/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.1505\n",
            "Epoch 177/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 1.0000 - loss: 0.1478\n",
            "Epoch 178/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.1452\n",
            "Epoch 179/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.1426\n",
            "Epoch 180/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 0.1397\n",
            "Epoch 181/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 1.0000 - loss: 0.1369\n",
            "Epoch 182/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 1.0000 - loss: 0.1341\n",
            "Epoch 183/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.1315\n",
            "Epoch 184/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 0.1291\n",
            "Epoch 185/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 0.1267\n",
            "Epoch 186/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 1.0000 - loss: 0.1245\n",
            "Epoch 187/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 1.0000 - loss: 0.1223\n",
            "Epoch 188/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.1201\n",
            "Epoch 189/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 0.1180\n",
            "Epoch 190/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 1.0000 - loss: 0.1159\n",
            "Epoch 191/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.1140\n",
            "Epoch 192/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.1122\n",
            "Epoch 193/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.1106\n",
            "Epoch 194/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.1094\n",
            "Epoch 195/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.1090\n",
            "Epoch 196/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 1.0000 - loss: 0.1104\n",
            "Epoch 197/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.1158\n",
            "Epoch 198/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.1378\n",
            "Epoch 199/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8696 - loss: 0.2733\n",
            "Epoch 200/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.5304 - loss: 1.1549\n",
            "Epoch 201/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.7826 - loss: 0.9103\n",
            "Epoch 202/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6957 - loss: 0.9625\n",
            "Epoch 203/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.9130 - loss: 0.3730\n",
            "Epoch 204/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7478 - loss: 0.5848\n",
            "Epoch 205/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8261 - loss: 0.4262\n",
            "Epoch 206/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7652 - loss: 0.4476\n",
            "Epoch 207/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.8609 - loss: 0.3822\n",
            "Epoch 208/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8696 - loss: 0.3546\n",
            "Epoch 209/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8348 - loss: 0.3594\n",
            "Epoch 210/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9391 - loss: 0.2303\n",
            "Epoch 211/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8957 - loss: 0.3050\n",
            "Epoch 212/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9304 - loss: 0.2508\n",
            "Epoch 213/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - accuracy: 0.9565 - loss: 0.2297\n",
            "Epoch 214/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.9913 - loss: 0.1974\n",
            "Epoch 215/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.9826 - loss: 0.2015\n",
            "Epoch 216/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9565 - loss: 0.2265\n",
            "Epoch 217/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.9652 - loss: 0.1751\n",
            "Epoch 218/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9565 - loss: 0.1891\n",
            "Epoch 219/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.9652 - loss: 0.1722\n",
            "Epoch 220/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.9913 - loss: 0.1719\n",
            "Epoch 221/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9913 - loss: 0.1652\n",
            "Epoch 222/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.9913 - loss: 0.1378\n",
            "Epoch 223/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 1.0000 - loss: 0.1391\n",
            "Epoch 224/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 1.0000 - loss: 0.1485\n",
            "Epoch 225/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.9913 - loss: 0.1377\n",
            "Epoch 226/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.1264\n",
            "Epoch 227/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.1212\n",
            "Epoch 228/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.1167\n",
            "Epoch 229/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.1145\n",
            "Epoch 230/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.1130\n",
            "Epoch 231/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.1089\n",
            "Epoch 232/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.1052\n",
            "Epoch 233/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.1028\n",
            "Epoch 234/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 1.0000 - loss: 0.1003\n",
            "Epoch 235/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 1.0000 - loss: 0.0991\n",
            "Epoch 236/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0964\n",
            "Epoch 237/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 0.0934\n",
            "Epoch 238/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.0922\n",
            "Epoch 239/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.0913\n",
            "Epoch 240/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 1.0000 - loss: 0.0886\n",
            "Epoch 241/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0855\n",
            "Epoch 242/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.0843\n",
            "Epoch 243/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.0837\n",
            "Epoch 244/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 0.0826\n",
            "Epoch 245/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.0812\n",
            "Epoch 246/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.0797\n",
            "Epoch 247/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.0782\n",
            "Epoch 248/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0770\n",
            "Epoch 249/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 1.0000 - loss: 0.0759\n",
            "Epoch 250/250\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.0747\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b64f836ead0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_sequence_accuracy(model, X, y, tokenizer):\n",
        "    total_tokens = 0\n",
        "    correct_tokens = 0\n",
        "\n",
        "    preds = model.predict(X, verbose=0)\n",
        "    pred_tokens = np.argmax(preds, axis=-1)\n",
        "\n",
        "    for true_seq, pred_seq in zip(y, pred_tokens):\n",
        "        for true_token, pred_token in zip(true_seq, pred_seq):\n",
        "            if true_token != 0:  # skip padding\n",
        "                total_tokens += 1\n",
        "                if true_token == pred_token:\n",
        "                    correct_tokens += 1\n",
        "\n",
        "    accuracy = (correct_tokens / total_tokens) * 100 if total_tokens > 0 else 0\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "crsJjLBcDCcG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate_sequence_accuracy(model, X, y, tokenizer)\n",
        "print(f\"Sequence Prediction Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAj6bg-5qvQ4",
        "outputId": "816c5ce7-fc5f-45fa-a36b-9391958e54e3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence Prediction Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_predicted_sequences(model, X, tokenizer):\n",
        "    preds = model.predict(X, verbose=0)\n",
        "    pred_token_ids = np.argmax(preds, axis=-1)\n",
        "\n",
        "    for i, pred_ids in enumerate(pred_token_ids):\n",
        "        pred_tokens = [tokenizer.index_word.get(id, '') for id in pred_ids]\n",
        "        true_tokens = [tokenizer.index_word.get(id, '') for id in y[i]]\n",
        "\n",
        "        print(f\"Example {i + 1}\")\n",
        "        print(f\"Predicted: {' '.join(pred_tokens)}\")\n",
        "        print(f\"Actual   : {' '.join(true_tokens)}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "print_predicted_sequences(model, X, tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWtPFdJIDQqH",
        "outputId": "0eba6e26-5e1f-4d18-efae-09d304a902d5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1\n",
            "Predicted: x * * 3 / 6 + x               \n",
            "Actual   : x * * 3 / 6 + x               \n",
            "==================================================\n",
            "Example 2\n",
            "Predicted: * * 4 / 24 + x * * 3 / 6 + x * * 2 / 2 + x + 1\n",
            "Actual   : * * 4 / 24 + x * * 3 / 6 + x * * 2 / 2 + x + 1\n",
            "==================================================\n",
            "Example 3\n",
            "Predicted: x * * 4 / 4 + x * * 3 / 3 - x * * 2 / 2 + x \n",
            "Actual   : x * * 4 / 4 + x * * 3 / 3 - x * * 2 / 2 + x \n",
            "==================================================\n",
            "Example 4\n",
            "Predicted: * * 4 / 24 - x * * 2 / 2 + 1         \n",
            "Actual   : * * 4 / 24 - x * * 2 / 2 + 1         \n",
            "==================================================\n",
            "Example 5\n",
            "Predicted: * * 3 / 3 + x                \n",
            "Actual   : * * 3 / 3 + x                \n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Specific Task 3:** Use Transformer model\n",
        "Please train a **Transformer**  model to learn the Taylor expansion of each function.\n"
      ],
      "metadata": {
        "id": "04XQJknfMSbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "input_sequences = [torch.tensor(seq[:-1]) for seq in padded_sequences]\n",
        "target_sequences = [torch.tensor(seq[1:]) for seq in padded_sequences]\n",
        "\n",
        "max_seq_length = max(len(seq) for seq in input_sequences)\n",
        "\n",
        "input_tensors = []\n",
        "target_tensors = []\n",
        "attention_masks = []\n",
        "\n",
        "for inp_seq, tgt_seq in zip(input_sequences, target_sequences):\n",
        "    inp_padded = torch.cat([inp_seq, torch.zeros(max_seq_length - len(inp_seq), dtype=torch.long)])\n",
        "    tgt_padded = torch.cat([tgt_seq, torch.zeros(max_seq_length - len(tgt_seq), dtype=torch.long)])\n",
        "\n",
        "    attention_mask = (inp_padded != 0).long()\n",
        "\n",
        "    input_tensors.append(inp_padded)\n",
        "    target_tensors.append(tgt_padded)\n",
        "    attention_masks.append(attention_mask)\n",
        "\n",
        "input_tensors = torch.stack(input_tensors)\n",
        "target_tensors = torch.stack(target_tensors)\n",
        "attention_masks = torch.stack(attention_masks)\n",
        "\n",
        "dataset = TensorDataset(input_tensors, target_tensors, attention_masks)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "id": "s0ajeO-oFUNT"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, nhead=8, num_layers=6, dim_feedforward=512, max_seq_length=50):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(max_seq_length, d_model))\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
        "        src = self.embedding(src) + self.positional_encoding[:src.size(1), :]\n",
        "        tgt = self.embedding(tgt) + self.positional_encoding[:tgt.size(1), :]\n",
        "\n",
        "        output = self.transformer(\n",
        "            src, tgt,\n",
        "            src_mask=src_mask, tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_key_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask\n",
        "        )\n",
        "\n",
        "        return self.fc_out(output)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "model = TransformerModel(vocab_size)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "zMwNvZRNpYTP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, num_epochs=150):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for src, tgt, attn_mask in dataloader:\n",
        "            src, tgt, attn_mask = src.to(device), tgt.to(device), attn_mask.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            loss = criterion(output.view(-1, vocab_size), tgt[:, 1:].reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "train(model, dataloader, criterion, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV0orZOaparC",
        "outputId": "d3623f04-5367-472c-ff1f-3936a2177229"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.011403001844882965\n",
            "Epoch 2, Loss: 0.010902094654738903\n",
            "Epoch 3, Loss: 0.011685309931635857\n",
            "Epoch 4, Loss: 0.010882451198995113\n",
            "Epoch 5, Loss: 0.010884811170399189\n",
            "Epoch 6, Loss: 0.010684220120310783\n",
            "Epoch 7, Loss: 0.010694213211536407\n",
            "Epoch 8, Loss: 0.010368735529482365\n",
            "Epoch 9, Loss: 0.010426792316138744\n",
            "Epoch 10, Loss: 0.010088512673974037\n",
            "Epoch 11, Loss: 0.010420485399663448\n",
            "Epoch 12, Loss: 0.009783769957721233\n",
            "Epoch 13, Loss: 0.009991672821342945\n",
            "Epoch 14, Loss: 0.011203566566109657\n",
            "Epoch 15, Loss: 0.010007910430431366\n",
            "Epoch 16, Loss: 0.009841850027441978\n",
            "Epoch 17, Loss: 0.009415388107299805\n",
            "Epoch 18, Loss: 0.00949324294924736\n",
            "Epoch 19, Loss: 0.009430560283362865\n",
            "Epoch 20, Loss: 0.009648923762142658\n",
            "Epoch 21, Loss: 0.00951122585684061\n",
            "Epoch 22, Loss: 0.009432496502995491\n",
            "Epoch 23, Loss: 0.009446363896131516\n",
            "Epoch 24, Loss: 0.009465762414038181\n",
            "Epoch 25, Loss: 0.00936500821262598\n",
            "Epoch 26, Loss: 0.009392991662025452\n",
            "Epoch 27, Loss: 0.009122245945036411\n",
            "Epoch 28, Loss: 0.009007859043776989\n",
            "Epoch 29, Loss: 0.008739311248064041\n",
            "Epoch 30, Loss: 0.008727195672690868\n",
            "Epoch 31, Loss: 0.008849029429256916\n",
            "Epoch 32, Loss: 0.008393890224397182\n",
            "Epoch 33, Loss: 0.008221332915127277\n",
            "Epoch 34, Loss: 0.0084680812433362\n",
            "Epoch 35, Loss: 0.008376605808734894\n",
            "Epoch 36, Loss: 0.008276810869574547\n",
            "Epoch 37, Loss: 0.008314106613397598\n",
            "Epoch 38, Loss: 0.008197875693440437\n",
            "Epoch 39, Loss: 0.008197505958378315\n",
            "Epoch 40, Loss: 0.008353346958756447\n",
            "Epoch 41, Loss: 0.008085711859166622\n",
            "Epoch 42, Loss: 0.007949844934046268\n",
            "Epoch 43, Loss: 0.007811458315700293\n",
            "Epoch 44, Loss: 0.007769728545099497\n",
            "Epoch 45, Loss: 0.00777002377435565\n",
            "Epoch 46, Loss: 0.00787372700870037\n",
            "Epoch 47, Loss: 0.007576527539640665\n",
            "Epoch 48, Loss: 0.00782473012804985\n",
            "Epoch 49, Loss: 0.007512929383665323\n",
            "Epoch 50, Loss: 0.007383592426776886\n",
            "Epoch 51, Loss: 0.007306480780243874\n",
            "Epoch 52, Loss: 0.007532229647040367\n",
            "Epoch 53, Loss: 0.007736539002507925\n",
            "Epoch 54, Loss: 0.007466768380254507\n",
            "Epoch 55, Loss: 0.007309285458177328\n",
            "Epoch 56, Loss: 0.007751806173473597\n",
            "Epoch 57, Loss: 0.007261819206178188\n",
            "Epoch 58, Loss: 0.00732165714725852\n",
            "Epoch 59, Loss: 0.007115044631063938\n",
            "Epoch 60, Loss: 0.006887116469442844\n",
            "Epoch 61, Loss: 0.007116297725588083\n",
            "Epoch 62, Loss: 0.006881681736558676\n",
            "Epoch 63, Loss: 0.006765533704310656\n",
            "Epoch 64, Loss: 0.006881189998239279\n",
            "Epoch 65, Loss: 0.006869770120829344\n",
            "Epoch 66, Loss: 0.006537853740155697\n",
            "Epoch 67, Loss: 0.00677208648994565\n",
            "Epoch 68, Loss: 0.006571099627763033\n",
            "Epoch 69, Loss: 0.006519012618809938\n",
            "Epoch 70, Loss: 0.006579131819307804\n",
            "Epoch 71, Loss: 0.006458471063524485\n",
            "Epoch 72, Loss: 0.006645914167165756\n",
            "Epoch 73, Loss: 0.006530623883008957\n",
            "Epoch 74, Loss: 0.006409434136003256\n",
            "Epoch 75, Loss: 0.006228073500096798\n",
            "Epoch 76, Loss: 0.006491826847195625\n",
            "Epoch 77, Loss: 0.006218177266418934\n",
            "Epoch 78, Loss: 0.0065356669947505\n",
            "Epoch 79, Loss: 0.006133250892162323\n",
            "Epoch 80, Loss: 0.006025965325534344\n",
            "Epoch 81, Loss: 0.005875237751752138\n",
            "Epoch 82, Loss: 0.00614182511344552\n",
            "Epoch 83, Loss: 0.005987204145640135\n",
            "Epoch 84, Loss: 0.006513910833746195\n",
            "Epoch 85, Loss: 0.005999932065606117\n",
            "Epoch 86, Loss: 0.006134494207799435\n",
            "Epoch 87, Loss: 0.0059824553318321705\n",
            "Epoch 88, Loss: 0.0060034883208572865\n",
            "Epoch 89, Loss: 0.0059296065010130405\n",
            "Epoch 90, Loss: 0.0058688814751803875\n",
            "Epoch 91, Loss: 0.0059966747649014\n",
            "Epoch 92, Loss: 0.0058007496409118176\n",
            "Epoch 93, Loss: 0.005787936970591545\n",
            "Epoch 94, Loss: 0.005729552358388901\n",
            "Epoch 95, Loss: 0.0056098210625350475\n",
            "Epoch 96, Loss: 0.005638275295495987\n",
            "Epoch 97, Loss: 0.005441299639642239\n",
            "Epoch 98, Loss: 0.005434574093669653\n",
            "Epoch 99, Loss: 0.0054471553303301334\n",
            "Epoch 100, Loss: 0.005620479583740234\n",
            "Epoch 101, Loss: 0.005381738767027855\n",
            "Epoch 102, Loss: 0.005292936228215694\n",
            "Epoch 103, Loss: 0.005557394120842218\n",
            "Epoch 104, Loss: 0.005442728288471699\n",
            "Epoch 105, Loss: 0.0053052050061523914\n",
            "Epoch 106, Loss: 0.0060219597071409225\n",
            "Epoch 107, Loss: 0.0054967207834124565\n",
            "Epoch 108, Loss: 0.005301103927195072\n",
            "Epoch 109, Loss: 0.005175521597266197\n",
            "Epoch 110, Loss: 0.005249648820608854\n",
            "Epoch 111, Loss: 0.004931296221911907\n",
            "Epoch 112, Loss: 0.005201367661356926\n",
            "Epoch 113, Loss: 0.00512299919500947\n",
            "Epoch 114, Loss: 0.004965719301253557\n",
            "Epoch 115, Loss: 0.005089770536869764\n",
            "Epoch 116, Loss: 0.005083614960312843\n",
            "Epoch 117, Loss: 0.004948973190039396\n",
            "Epoch 118, Loss: 0.005054616369307041\n",
            "Epoch 119, Loss: 0.004841612186282873\n",
            "Epoch 120, Loss: 0.004669151734560728\n",
            "Epoch 121, Loss: 0.004905519541352987\n",
            "Epoch 122, Loss: 0.004921883810311556\n",
            "Epoch 123, Loss: 0.004759583622217178\n",
            "Epoch 124, Loss: 0.004804203752428293\n",
            "Epoch 125, Loss: 0.004638882353901863\n",
            "Epoch 126, Loss: 0.004638850223273039\n",
            "Epoch 127, Loss: 0.004574175458401442\n",
            "Epoch 128, Loss: 0.004691298119723797\n",
            "Epoch 129, Loss: 0.004608858376741409\n",
            "Epoch 130, Loss: 0.004472014959901571\n",
            "Epoch 131, Loss: 0.004635822027921677\n",
            "Epoch 132, Loss: 0.004636678844690323\n",
            "Epoch 133, Loss: 0.004519424866884947\n",
            "Epoch 134, Loss: 0.004539265763014555\n",
            "Epoch 135, Loss: 0.004607728682458401\n",
            "Epoch 136, Loss: 0.0043059466406702995\n",
            "Epoch 137, Loss: 0.004562489688396454\n",
            "Epoch 138, Loss: 0.004520062357187271\n",
            "Epoch 139, Loss: 0.004501312971115112\n",
            "Epoch 140, Loss: 0.0045350356958806515\n",
            "Epoch 141, Loss: 0.0044078766368329525\n",
            "Epoch 142, Loss: 0.004653407726436853\n",
            "Epoch 143, Loss: 0.004484421573579311\n",
            "Epoch 144, Loss: 0.004209138453006744\n",
            "Epoch 145, Loss: 0.0044922237284481525\n",
            "Epoch 146, Loss: 0.004319129977375269\n",
            "Epoch 147, Loss: 0.004250429105013609\n",
            "Epoch 148, Loss: 0.004228577483445406\n",
            "Epoch 149, Loss: 0.004135802388191223\n",
            "Epoch 150, Loss: 0.004082707688212395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_sequence_accuracy(model, dataloader, tokenizer):\n",
        "    model.eval()\n",
        "    total_tokens = 0\n",
        "    correct_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt, attn_mask in dataloader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            output = model(src, tgt[:, :-1])\n",
        "            predictions = output.argmax(dim=-1)\n",
        "\n",
        "            true_seq = tgt[:, 1:]\n",
        "            mask = true_seq != 0\n",
        "            correct = (predictions == true_seq) & mask\n",
        "            correct_tokens += correct.sum().item()\n",
        "            total_tokens += mask.sum().item()\n",
        "\n",
        "    accuracy = (correct_tokens / total_tokens) * 100 if total_tokens > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "accuracy = evaluate_sequence_accuracy(model, dataloader, tokenizer)\n",
        "print(f\"\\n Sequence Prediction Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5kgdz8CuV70",
        "outputId": "d657016b-5860-4cb3-ca6c-492ac1be9d98"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Sequence Prediction Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g4PPzPuBw7Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1FDlQnDNw77B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}